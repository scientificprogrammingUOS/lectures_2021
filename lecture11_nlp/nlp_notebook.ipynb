{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L11 - Natural Language Processing\n",
    "\n",
    "Parts of this notebook were adapted from Chapters [15](https://github.com/cltl/python-for-text-analysis/blob/master/Chapters/Chapter%2015%20-%20Off%20to%20analyzing%20text.ipynb) and [19](https://github.com/cltl/python-for-text-analysis/blob/master/Chapters/Chapter%2019%20-%20More%20about%20Natural%20Language%20Processing%20Tools%20(spaCy).ipynb) of VU Amsterdam's [Python for Text Analysis](https://github.com/cltl/python-for-text-analysis) course.\n",
    "\n",
    "Natural Language Processing (NLP) is a field of computational science dealing with natural (human-generated) language texts. Python has a wide range of NLP libraries, the most popular of which are perhaps [NLTK](https://www.nltk.org/) and [SpaCy](https://spacy.io/). We will focus on them in this lecture, but they are not the only game in town:\n",
    "\n",
    "* [Stanford's CoreNLP](http://stanfordnlp.github.io/CoreNLP/) is a very powerful system\n",
    "  that is able to process English, German, Spanish, French, Chinese and Arabic - it is originally written in Java, but there are also Python wrappers available, such as [py-corenlp](https://github.com/smilli/py-corenlp)\n",
    "* [Textblob](http://textblob.readthedocs.io/en/dev/) is a general NLP library that builds on NLTK\n",
    "* [Gensim](http://radimrehurek.com/gensim/) is mostly used for training semantic word vectors\n",
    "* [Corpkit](http://corpkit.readthedocs.io/en/latest/) is a module for corpus building and corpus management. Includes an interface to the Stanford CoreNLP parser\n",
    "* [Huggingface's Transformers](https://huggingface.co/transformers/) is considered state-of-the-art when working with [Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) models - supports both [TensorFlow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing `spacy` and `nltk` requires a bit more effort than we are used to from other packages. This is the case because both require the downloaded resources to function well and (in the case of `spacy`) are optimized for speedy performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to install `spacy` by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==3.0.6\n",
    "\n",
    "import spacy\n",
    "print(\"Yay, it worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this didn't work, here are a few steps you can try in a terminal with your environment activated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try upgrading `pip` with `pip install --upgrade pip`\n",
    "2. Try running `pip install numpy==1.19.5` (if `numpy` is not already installed)\n",
    "3. Try running `pip install Cython==0.29.23` (if `Cython` is not already installed)\n",
    "4. Try installing `wheel` with `pip install wheel`\n",
    "5. Try installing `setuptools` with `pip install setuptools`\n",
    "6. Finally, try runnning `pip install spacy==3.0.6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a quite comprehensive installation guide for different OS from `spacy` itself [here](https://spacy.io/usage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can use `spacy`, you have to download the trained models and resources for the language that you want to work with. `spacy` has models for a wider range of languages such as Chinese, Greek, Lithuanian, German, Japanese, Russian and many more. We will use the smallest English resource set `en_core_web_sm`. To download it, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try loading the downloaded resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "`nltk` should be easier to set up, since unlike `spacy` it does not use [Cython](https://cython.org/) for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk==3.6.2\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` does not have a single resource set for a language, but you need to download the resources for each step that you want to do. Let's start with all resources for `nltk`'s primary tutorial, the [NLTK book](https://www.nltk.org/book/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is really just a collection of tasks that operate on natural language. There's different motivations for solving these tasks: \n",
    "\n",
    "**Researchers** often want to solve them because they believe a system that can correctly reason over a text (e.g. answer questions) necessarily has to have some sort of \"understanding\" of its content. In recent years, neural networks have solved tasks like question answering to such a high degree that some say we have succeeded in teaching computers Natural Language Understanding (NLU) - however, [many others disagree and urge to rethink the approach the NLP community has taken towards understanding](https://www.aclweb.org/anthology/2020.acl-main.463/).\n",
    "\n",
    "**Companies** often want to solve these tasks because it enables them to provide a service to their customers, such as a chatbot, a voice assistant, a translation service, or detailed analysis of documents. Large companies like [Google](https://research.google/research-areas/natural-language-processing/) sometimes also publish research on NLP because they want to advance the community and add to their own reputation by achieving state-of-the-art results.\n",
    "\n",
    "The tasks range from trivial for humans to very difficult and often build up on each other (e.g. many tasks operate on tokens). Generally, one can distinguish between **text analysis**, where some information is drawn from an existing text, and **text generation**, where a new text is produced.\n",
    "\n",
    "\n",
    "Here are some low-level tasks, which often produce features that are the basis for more sophisticated tasks:\n",
    "\n",
    "* **Tokenization:** splitting texts into individual words (tokens)\n",
    "* **Sentence splitting:** splitting texts into sentences\n",
    "* **Part-of-speech (POS) tagging:** identifying the parts of speech of words in context (verbs, nouns, adjectives, etc.)\n",
    "* **Morphological analysis:** separating words into morphemes and identifying their classes (e.g. tense/aspect of verbs)\n",
    "* **Stemming:** identifying the stems of words in context by removing inflectional/derivational affixes, such as 'troubl' for 'trouble/troubling/troubled'\n",
    "* **Lemmatization:** identifying the lemmas (dictionary forms) of words in context, such as 'go' for 'go/goes/going/went'\n",
    "* **Word Sense Disambiguation (WSD):** assigning the correct meaning to words in context\n",
    "* **Stop words recognition:** identifying commonly used words (such as 'the', 'a(n)', 'in', etc.) in text, possibly to ignore them in other tasks\n",
    "* **Named Entity Recognition (NER):** identifying people, locations, organizations, etc. in text\n",
    "* **Constituency/dependency parsing:** analyzing the grammatical structure of a sentence\n",
    "* **Semantic Role Labeling (SRL):** analyzing the semantic structure of a sentence (*who* does *what* to *whom*, *where* and *when*)\n",
    "* **Sentiment Analysis:** determining whether a text is mostly positive or negative\n",
    "* **Static Word Representation and Semantic Similarity:** representating the meaning of words as rows of real valued numbers where each point captures a dimension of the word's meaning and where semantically similar words have similar vectors (very popular these days)\n",
    "* **Contextualized Word Representation:** representing the meaning of a word use in the context in which it was used\n",
    "\n",
    "Here are some high-level tasks, which are solved based on features produced by low-level tasks:\n",
    "* **Question Answering:** generating an answer to a question\n",
    "* **Common-Sense Reasoning:** reasoning over the contents of a text\n",
    "* **Machine Translation:** generating a translation in a different natural language\n",
    "* **Paraphrase Generation:** generating a text of similar length with the same meaning\n",
    "* **Abstractive Summarization:** Generate a text of shorter length that summarizes a text\n",
    "\n",
    "This list is non-comprehensive. If you want to learn more about these and other high-level topics, have a look at [nlpprogress.com](http://nlpprogress.com/), which has a summary of each task and the methods that currently perform best on it for many languages.  \n",
    "\n",
    "If you want to see a demonstration of what state-of-the-art NLP models can do, check out this [text autocompletion demo with GPT-2](https://transformer.huggingface.co/doc/gpt2-large) by Huggingface and this [DALL-E text-to-image  demo](https://openai.com/blog/dall-e/) by OpenAI, which is not strictly NLP but also connected with computer vision as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLP Pipeline with NLTK\n",
    "\n",
    "Let's look at an example of a simple NLP pipeline with `nltk`. In the following cell, you can observe how we tokenize raw text into tokens and sentences, perform part of speech tagging and lemmatize some of the tokens. Don't worry about the details just yet - we will go through them step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"I have an awesome cat. It's sitting on the mat that I bought yesterday.\"\n",
    "\n",
    "# tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# sentence splitting\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# POS tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# lemmatization\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "lemma=lmtzr.lemmatize(tokens[4], 'v')\n",
    "\n",
    "# print all information\n",
    "print(tokens)\n",
    "print(sentences)\n",
    "print(tagged_tokens)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Sentence Splitting\n",
    "#### `word_tokenize()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try tokenizing a story from a text file in `data/charlie.txt`. First, we will open and read the file and assign the file contents to the variable `content`. If you are unsure about how to read and write files in Python, have a look at [Chapter 14 of the Python for Text Analyis course](https://github.com/cltl/python-for-text-analysis/blob/master/Chapters/Chapter%2014%20-%20Reading%20and%20writing%20text%20files.ipynb). Then, we can call the `word_tokenize()` function from the `nltk` module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "\n",
    "tokens = nltk.word_tokenize(content)\n",
    "print(type(tokens), len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we now have a list of all words in the text. The punctuation marks are also in the list, but as separate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sent_tokenize()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that NLTK can do for you is to split a text into sentences by using the `sent_tokenize()` function. We use it on the entire text (as a string):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(content)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do all sorts of cool things with these lists. For example, we can search for all words that have certain letters in them and add them to a list. Let's say we want to find all present participles in the text. We know that present participles end with *-ing*, so we can do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read in file as a string, assign it to the variable `content`\n",
    "with open(\"data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "# split up entire text into tokens using word_tokenize():\n",
    "tokens = nltk.word_tokenize(content)\n",
    "\n",
    "# create an empty list to collect all words having the present participle -ing:\n",
    "present_participles = []\n",
    "\n",
    "# looking through all tokens\n",
    "for token in tokens:\n",
    "    # checking if a token ends with the present parciciple -ing\n",
    "    if token.endswith(\"ing\"):\n",
    "        # if the condition is met, add it to the list we created above (present_participles)\n",
    "        present_participles.append(token)\n",
    "        \n",
    "# ürint the list to inspect it\n",
    "print(present_participles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good! We now have a list of words like *boiling*, *sizzling*, etc. However, we can see that there is one word in the list that actually is not a present participle (*ceiling*). Of course, also other words can end with *-ing*. So if we want to find all present participles, we have to come up with a smarter solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, `nltk` comes to the rescue. Using the function `pos_tag()`, we can label each word in the text with its part of speech. \n",
    "\n",
    "To do pos-tagging, you first need to tokenize the text. We have already done this above, but we will repeat the steps here, so you get a sense of what an NLP pipeline may look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `pos_tag()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how `pos_tag()` can be used, we can (as always) look at the documentation by using the `help()` function. As we can see, `pos_tag()` takes a tokenized text as input and returns a list of tuples in which the first element corresponds to the token and the second to the assigned pos-tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always, we can start by reading the documentation:\n",
    "help(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read in file as a string, assign it to the variable `content`\n",
    "with open(\"data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "# split up entire text into tokens using word_tokenize():\n",
    "tokens = nltk.word_tokenize(content)\n",
    "\n",
    "# apply pos tagging to the tokenized text\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# inspect pos tags\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, `pos_tag()` returns a list of tuples: The first element is the token, the second element indicates the part of speech (POS) of the token. \n",
    "\n",
    "This POS tagger uses the POS tag set of the Penn Treebank Project, which can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). For example, all tags starting with a V are used for verbs. \n",
    "\n",
    "We can now use this, for example, to identify all the verbs in a text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read in file as a string, assign it to the variable `content`\n",
    "with open(\"data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "# apply tokenization and POS tagging\n",
    "tokens = nltk.word_tokenize(content)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# list of verb tags (i.e. tags we are interested in)\n",
    "verb_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "\n",
    "# create an empty list to collect all verbs:\n",
    "verbs = []\n",
    "\n",
    "# iterating over all tagged tokens\n",
    "for token, tag in tagged_tokens:\n",
    " \n",
    "    # checking if the tag is any of the verb tags\n",
    "    if tag in verb_tags:\n",
    "        # if the condition is met, add it to the list we created above \n",
    "        verbs.append(token)\n",
    "        \n",
    "# print the list to inspect it\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `nltk` to lemmatize words.\n",
    "\n",
    "The lemma of a word is the form of the word which is usually used in dictionary entries. This is useful for many NLP tasks, as it gives a better generalization than the strong a word appears in. To a computer, `cat` and `cats` are two completely different tokens, even though we know they are both forms of the same lemma. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The WordNet lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a kind of lemmatizer called `WordNetLemmatizer` for this using the `lemmatize()` function. [WordNet](https://en.wikipedia.org/wiki/WordNet) is an ontology-like lexical database of English words and their semantic relations. In the code below, we loop through the list of verbs, lemmatize each of the verbs, and add them to a new list called `verb_lemmas`. Again, we show all the processing steps (consider the comments in the code below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "tokens = nltk.word_tokenize(content)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "verb_tags = [\"VBD\", \"VB\", \"G\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "verbs = []\n",
    "\n",
    "for token, tag in tagged_tokens:\n",
    "    if tag in verb_tags:\n",
    "        verbs.append(token)\n",
    "\n",
    "print(verbs)\n",
    "\n",
    "# instatiate a lemmatizer object\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# create list to collect all the verb lemmas:\n",
    "verb_lemmas = []\n",
    "        \n",
    "for participle in verbs:\n",
    "    \n",
    "    # for this lemmatizer, we need to indicate the POS of the word (in this case, v = verb)\n",
    "    lemma = lmtzr.lemmatize(participle, \"v\") \n",
    "    verb_lemmas.append(lemma)\n",
    "    \n",
    "print(verb_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about the this lemmatizer:** \n",
    "\n",
    "We need to specify a POS tag to the `WordNetLemmatizer`, in a `WordNet` format (\"n\" for noun, \"v\" for verb, \"a\" for adjective). If we do not indicate the POS tag, the WordNet lemmatizer thinks it is a noun (this is the default value for its part-of-speech). See the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nouns = ('building', 'applications', 'leafs')\n",
    "for n in test_nouns:\n",
    "    print(f\"Noun in conjugated form: {n}\")\n",
    "    default_lemma = lmtzr.lemmatize(n) # without specifying POS n is interpreted as a noun!\n",
    "    print(f\"Default lemmatization: {default_lemma}\")\n",
    "    verb_lemma = lmtzr.lemmatize(n, 'v')\n",
    "    print(f\"Lemmatization as a verb: {verb_lemma}\")\n",
    "    noun_lemma = lmtzr.lemmatize(n, 'n')\n",
    "    print(f\"Lemmatization as a noun: {noun_lemma}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_verbs = ('grew', 'standing', 'plays')\n",
    "\n",
    "for v in test_verbs:\n",
    "    print(f\"Verb in conjugated form: {v}\")\n",
    "    default_lemma = lmtzr.lemmatize(v) # without specifying POS v is interpreted as a noun!\n",
    "    print(f\"Default lemmatization: {default_lemma}\")\n",
    "    verb_lemma = lmtzr.lemmatize(v, 'v')\n",
    "    print(f\"Lemmatization as a verb: {verb_lemma}\")\n",
    "    noun_lemma = lmtzr.lemmatize(v, 'n')\n",
    "    print(f\"Lemmatization as a noun: {noun_lemma}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ontological Semantics: WordNet Synsets\n",
    "\n",
    "All the steps that we have done with `nltk` so far - **sentence splitting**, **tokenization**, **POS tagging**, and **lemmatization** - are all very nice, but arguably **they only scratch at the surface** of the actual meaning of the text.\n",
    "\n",
    "What we would really be interested in is learning about the [semantics](https://en.wikipedia.org/wiki/Semantics) of the text, the actual content. This is the holy grail of NLP that is still the subject of a lot of research. However, one way to get some idea what the text is about is looking up the words in the lexical database [WordNet](https://wordnet.princeton.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In WordNet, English nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms, so-called *synsets*, each expressing a distinct concept. That way, it is possible to look up which words could in general \"mean\" the same concept, although that is not a guarantee that they do mean the same thing in this specific context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(wordnet.synsets(\"dish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This given you a list of `Synset` objects that denote concepts which might be meant by the word \"pole\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wordnet.synsets(\"dog\")\n",
    "\n",
    "print(type(synsets[0]))\n",
    "print(synsets[0].name())\n",
    "print(type(synsets[0].name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Synset.name()` gives you the name as a `str`, but there's much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synsets[0].definition())\n",
    "print(synsets[0].examples())\n",
    "print(synsets[0].lemmas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you would like to know if two words could possibly mean the same thing, you could check for intersection of the synsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = wordnet.synsets(\"play\")\n",
    "b = wordnet.synsets(\"drama\")\n",
    "\n",
    "a_synset_names = {synset.name() for synset in a}\n",
    "b_synset_names = {synset.name() for synset in b}\n",
    "\n",
    "synsets_of_both = a_synset_names.intersection(b_synset_names)\n",
    "print(synsets_of_both)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it can sometimes be sensible to narrow down the search space by giving a POS tag, as in the case of \"play\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of synsets without POS:\", len(wordnet.synsets(\"play\")))\n",
    "print(\"Number of synsets with POS:\", len(wordnet.synsets(\"play\", pos=\"v\"))) # v stands for verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot more to the WordNet functionality of `nltk`. To learn more, see [this](https://www.nltk.org/howto/wordnet.html) how-to.\n",
    "\n",
    "If you would like to learn more about `nltk` in general, check out its [documentation](https://www.nltk.org/api/nltk.html) and the [book](https://www.nltk.org/book/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLP Pipeline with SpaCy\n",
    "\n",
    "`spacy` has mostly the same functions that `nltk` has, but they are accessed with a different syntax. First, a set of pretrained models is loaded and stored in a `nlp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nlp` object can be used to perform all steps of the NLP pipeline at once on a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have an awesome cat. It's sitting on the mat that I bought yesterday.\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc` is now a Python object of the class `Doc`. It is a container for accessing linguistic annotations and a sequence of `Token` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc, Token and Span Objects\n",
    "\n",
    "At this point, there are three important types of objects to remember:\n",
    "\n",
    "* A `Doc` is a sequence of `Token` objects.\n",
    "* A `Token` object represents an individual token — i.e. a word, punctuation symbol, whitespace, etc. It has attributes representing linguistic annotations. \n",
    "* A `Span` object is a slice from a `Doc` object and a sequence of `Token` objects.\n",
    "\n",
    "Since `Doc` is a sequence of `Token` objects, we can iterate over all of the tokens in the text as shown below, or select a single token from the sequence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the tokens\n",
    "for token in doc:\n",
    "    print(token)\n",
    "print()\n",
    "\n",
    "# select one single token by index\n",
    "first_token = doc[0]\n",
    "print(\"First token:\", first_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linguistic features are then available as attributes using an object-oriented syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(\"\\t\".join([token.text, token.pos_, token.lemma_, token.shape_, token.dep_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full list of linguistic features can be found [here](https://spacy.io/usage/linguistic-features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some of the attributes end with an underscore. For example, tokens have both `lemma` and `lemma_` attributes. The `lemma` attribute represents the id of the lemma (integer), while the `lemma_` attribute represents the unicode string representation of the lemma. In practice, you will mostly use the `lemma_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `spacy.explain` to find out more about certain labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"npadvmod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `Span` object from the slice `doc[start : end]`. For instance,`doc[2:5]` produces a span consisting of tokens 2, 3 and 4. Stepped slices (e.g. `doc[start : end : step]`) are not supported, as `Span` objects must be contiguous (cannot have gaps). You can use negative indices and open-ended ranges, which have their normal Python semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a span\n",
    "a_slice = doc[2:5]\n",
    "print(a_slice, type(a_slice))\n",
    "\n",
    "# iterate over span\n",
    "for token in a_slice:\n",
    "    print(token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc Attributes\n",
    "\n",
    "If you call the `dir()` function on a `Doc` object, you will see that it has a range of methods and attributes. You can read more about them in the [documentation](https://spacy.io/api/doc). Below, we highlight two of them: `text` and `sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, `text` simply gives you the whole document as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.text)\n",
    "print(type(doc.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sents` can be used to get all the sentences. Notice that it will create a so-called 'generator'. For now, you don't have to understand exactly what a generator is (if you like, you can read more about them online). Just remember that we can use generators to iterate over an object in a fast and efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the sentences as a generator \n",
    "print(doc.sents, type(doc.sents))\n",
    "\n",
    "# we can use the generator to loop over the sentences; each sentence is a span of tokens\n",
    "for sentence in doc.sents:\n",
    "    print(sentence, type(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find this difficult to comprehend, you can also simply convert it to a list and then loop over the list. Remember that this is less efficient, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also store the sentences in a list and then loop over the list \n",
    "sentences = list(doc.sents)\n",
    "for sentence in sentences:\n",
    "    print(sentence, type(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of converting it to a list is that we can use indices to select certain sentences. For example, in the following we only print some information about the tokens in the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print some information about the tokens in the second sentence.\n",
    "sentences = list(doc.sents)\n",
    "for token in sentences[1]:\n",
    "    data = '\\t'.join([token.orth_,\n",
    "                      token.lemma_,\n",
    "                      token.pos_,\n",
    "                      token.tag_,\n",
    "                      str(token.i),    # turn index into string\n",
    "                      str(token.idx)]) # turn index into string\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you deduce from this output what the difference between `Token.i` and `Token.idx` is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "`spacy` also automatically runs a Named Entity Recognition algorithm over the text when `nlp` is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a slightly longer text from the Wikipedia page about Harry Potter\n",
    "harry_potter = \"Harry Potter is a series of fantasy novels written by British author J. K. Rowling.\\\n",
    "The novels chronicle the life of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley,\\\n",
    "all of whom are students at Hogwarts School of Witchcraft and Wizardry.\\\n",
    "The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal,\\\n",
    "overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles.\"\n",
    "\n",
    "doc = nlp(harry_potter)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, you can simply access all entities that are mentioned in a `Doc` with `Doc.ents`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Semantics: GloVe Word Vectors\n",
    "\n",
    "Just like `nltk` has Wordnet synsets, `spacy` also has an interface that lets you access a semantic information about each token. `spacy` makes pretrained semantic numeric vectors for each token available via the [`Token.vector`](https://spacy.io/api/token#vector) attribute. In the case of `en_core_web_sm`, these vectors are 300-dimensional [GloVe vectors](https://nlp.stanford.edu/projects/glove/). These vectors are trained from massive corpora and encode word co-occurrence statistics. Here's how to access them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harry = doc[0]\n",
    "\n",
    "print(harry.text)\n",
    "print(harry.vector)\n",
    "print(type(harry.vector))\n",
    "print(harry.vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, they are simply `np.ndarray`s, which we already know how to handle. These vectors could be used e.g. as input to a neural network that learns to solve a NLP task such as e.g. sentiment analysis, the task of correctly classifying the sentiment expressed in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to know more about where these word vectors come form and how they are trained, check out [this](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) blog post by Chris Olah from 2014. It might be a bit outdated, but I think it's still a good way to get started with semantic word representations.\n",
    "\n",
    "If you would like to learn more about `spacy` in general, check out its [documentation](https://spacy.io/usage/linguistic-features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "You can find the assignment link for `2021-homework11` in an announcement on StudIP. It will make use of both `nltk` and `spacy`, so your time scrolling through this notebook was not wasted.\n",
    "\n",
    ">Good luck and have a good week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1413b6ca63b841d29f19b2c908ba8ea8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1427ca6da23d47e88347e3ae5a8acd6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "CheckboxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "CheckboxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "CheckboxView",
       "description": "sine1",
       "description_tooltip": null,
       "disabled": false,
       "indent": true,
       "layout": "IPY_MODEL_53d391da1d1f41b3a5e38d7cdd851d83",
       "style": "IPY_MODEL_401319c8166044d8af79243728432b9b",
       "value": false
      }
     },
     "2ee712068d8b403fa4b8ebae194bee76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "401319c8166044d8af79243728432b9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "53d391da1d1f41b3a5e38d7cdd851d83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5c2d9b1b67764d4ca56451c7098e5330": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": true,
       "description": "freq",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_1413b6ca63b841d29f19b2c908ba8ea8",
       "max": 40,
       "min": 1,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.1,
       "style": "IPY_MODEL_8baffec77bd64355bdd3c6775cc06376",
       "value": 1
      }
     },
     "6bc77fdc23fc458d98ef353aa7e0078e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6d463b4ec6564e2090a1cfbb11469e24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8baffec77bd64355bdd3c6775cc06376": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "945af3619e724e4aa3b939fcabe58780": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e912247a7084ac0a41071290ba49232": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "CheckboxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "CheckboxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "CheckboxView",
       "description": "sine3",
       "description_tooltip": null,
       "disabled": false,
       "indent": true,
       "layout": "IPY_MODEL_945af3619e724e4aa3b939fcabe58780",
       "style": "IPY_MODEL_ec2d0c6e7c0e4091a785761257c3823e",
       "value": false
      }
     },
     "9f46286059a14439a31cecb0dc837b1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "CheckboxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "CheckboxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "CheckboxView",
       "description": "sine2",
       "description_tooltip": null,
       "disabled": false,
       "indent": true,
       "layout": "IPY_MODEL_2ee712068d8b403fa4b8ebae194bee76",
       "style": "IPY_MODEL_6bc77fdc23fc458d98ef353aa7e0078e",
       "value": false
      }
     },
     "bf5e9188dab34027bbf3fe7d8d263d35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3bf0c534a9c4e489fab104eee1b4c82": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_bf5e9188dab34027bbf3fe7d8d263d35",
       "msg_id": "",
       "outputs": []
      }
     },
     "c4a09fd1730b47119334c530902b033b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5c2d9b1b67764d4ca56451c7098e5330",
        "IPY_MODEL_1427ca6da23d47e88347e3ae5a8acd6e",
        "IPY_MODEL_9f46286059a14439a31cecb0dc837b1d",
        "IPY_MODEL_9e912247a7084ac0a41071290ba49232",
        "IPY_MODEL_c3bf0c534a9c4e489fab104eee1b4c82"
       ],
       "layout": "IPY_MODEL_6d463b4ec6564e2090a1cfbb11469e24"
      }
     },
     "ec2d0c6e7c0e4091a785761257c3823e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
